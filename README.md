ğŸ§  Chatbot LLM Offline para Raspberry Pi

Este projeto permite executar um chatbot com Modelos de Linguagem de Grande Escala (LLMs) diretamente em um Raspberry Pi 5, sem necessidade de conexÃ£o com a internet apÃ³s a instalaÃ§Ã£o.  Utiliza modelos quantizados no formato GGUF via llama.cpp, oferecendo interfaces tanto em terminal quanto web. 


---

ğŸš€ Funcionalidades

âœ… ExecuÃ§Ã£o local de modelos LLM quantizados (GGUF) com llama.cpp.

âœ… Interfaces de uso via terminal e web (FastAPI + Jinja2).

âœ… CompatÃ­vel com modelos como Mistral, LLaMA, entre outros.

âœ… OperaÃ§Ã£o offline apÃ³s configuraÃ§Ã£o inicial.

âœ… Ideal para dispositivos com recursos limitados, como o Raspberry Pi 5. 



---

ğŸ§° Requisitos

Raspberry Pi 5 com Raspberry Pi OS 64-bit.

Python 3.11 ou superior.

MÃ­nimo de 8GB de RAM.

Acesso Ã  internet para a instalaÃ§Ã£o inicial. 



---

âš™ï¸ InstalaÃ§Ã£o

1. Atualize o sistema e instale dependÃªncias:

sudo apt update && sudo apt install -y build-essential cmake python3-dev python3-venv git





2. Clone o repositÃ³rio e configure o ambiente virtual:

git clone https://github.com/emeryinlover/srvllm.git
cd srvllm
python3 -m venv .venv
source .venv/bin/activate
pip install -U pip
pip install fastapi uvicorn jinja2 llama-cpp-python





3. Compile o llama.cpp:

git clone https://github.com/ggerganov/llama.cpp.git
cd llama.cpp && make && cd ..





4. Baixe um modelo quantizado (exemplo: Mistral Q4_K):

mkdir -p model && cd model
wget https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF/resolve/main/mistral-7b-instruct-v0.1.Q4_K_M.gguf
cd ..






---

ğŸ§ª Uso

Terminal

Execute o chatbot no terminal com: 

python app/cli.py



Interface Web

Inicie o servidor web com: 

./start.sh



Acesse via navegador em: http://<ip-do-raspberry>:8000 


---

ğŸ“ Estrutura do Projeto

srvllm/

â”œâ”€â”€ app/
â”‚
â”‚ â”€â”€â”œâ”€â”€ cli.py
â”‚   â”‚
â”‚   â”œâ”€â”€ main.py
â”‚   â”‚
â”‚   â””â”€â”€ templates/
â”‚   
â”œâ”€â”€ model/
â”‚   â”‚
â”‚   â””â”€â”€ mistral-7b-instruct-v0.1.Q4_K_M.gguf
â”‚   
â”œâ”€â”€ llama.cpp/
â”‚   
â”œâ”€â”€ start.sh
â”‚   
â””â”€â”€ README.md





---

ğŸ“Œ Notas

Modelos quantizados no formato GGUF sÃ£o otimizados para dispositivos com recursos limitados.

Ã‰ possÃ­vel utilizar outros modelos compatÃ­veis com llama.cpp, como LLaMA 2, Falcon, entre outros.

Para desempenho ideal, recomenda-se o uso de um Raspberry Pi 5 com 8GB de RAM. 



---

ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Sinta-se Ã  vontade para abrir issues ou pull requests para melhorias, correÃ§Ãµes ou novas funcionalidades. 


---

ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License. 


---

Para mais informaÃ§Ãµes sobre llama.cpp e modelos quantizados, consulte os seguintes recursos: 

RepositÃ³rio do llama.cpp: https://github.com/ggerganov/llama.cpp

Modelos quantizados GGUF no Hugging Face: https://huggingface.co/TheBloke 



---

Espero que este README.md atenda Ã s suas expectativas e facilite o uso e compreensÃ£o do projeto srvllm!
